---
title: "Political A/B Testing: Homepage Conversion Analysis"
description: |
  An analysis of simulated A/B test data for a political party's website, comparing Linear Probability and Logistic Regression Models.
author:
  - name: Holly Lloyd
date: 2026-02-13
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
    self_contained: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
set.seed(2024)

library(ggplot2)
library(tidyverse)
library(scales)
library(knitr)
library(broom)
# Loading in packages
```

# Overview

In this showcase, I will work with a simulated A/B test run by a fictitious **political party** delivering messages on its official website.

The party wants to increase the probability that visitors **sign up as supporters**. 

To do this, they are going to test two homepage versions:

1. **Control homepage (Version A)**: 
  Standard party messaging (policy highlights, leader photo, "Join us" button).
2. **Issue-focused homepage (Version B)**: 
  Emphasizes a hot-button issue, such as immigration, important to the party’s base, plus a "Join us" button.

Visitors are **randomly assigned** to Version A or Version B when they land on the website.

The outcome variable is:

- `support_signup = 1` if the website visitor signs up as a party-supporter
- `support_signup = 0` otherwise

In this showcase, I will:

1. Simulate the data for this A/B test
2. Explore and visualise the sign-up rates for each homepage
3. Estimate the Average Treatment Effect (ATE) using:
   - A difference-in-means calculation
   - A linear probability model (`lm`)
   - A logistic regression (`glm` with logit link)
4. Compare the models and interpret the results
5. Interpret heterogeneity by using an interaction 
6. Reflect briefly on ethical issues in running experiments in politics

---

#  Simulating the A/B Test Data 

I will simulate data for n = 1500 website visitors.

Assuming:

- Baseline signup probability under the control homepage is **0.12**.
- The issue-focused homepage increases signup probability by **0.05**, to **0.17**.

##  Code: Simulate the experiment

```{r simulate-data}
# 1. Set sample size
n <- 1500 

# 2. Random treatment assignment
treatment <- rbinom(n = n, size = 1, prob = 0.5)
# Assigning groups to 1 or 0 with 0.5 probability

# 3. Potential outcomes
p_control <- 0.12
p_treated <- 0.17

Y0 <- rbinom(n = n, size = 1, prob = p_control) 
# Simulates individual outcomes of control group
Y1 <- rbinom(n = n, size = 1, prob = p_treated)
# Simulates individual outcomes of treated group

# 4. Realised outcome
afford <- ifelse(treatment == 1, Y1, Y0)

# 5. Data frame

party_ab <- data.frame(
  visitor_id     = 1:n,
  treatment      = treatment,
  support_signup = afford
)
```

##  Inspect the data

```{r inspect-data}
party_abhead <- head(party_ab)
knitr::kable(party_abhead)

party_absummary <- summary(party_ab)
knitr::kable(party_absummary)

# Additional data inspection

party_abcount <- count(party_ab, treatment, support_signup)

party_abcount $ treatment <- ifelse(party_abcount $ treatment == 0, "Control", "Treatment")
party_abcount $ support_signup <- ifelse(party_abcount $ support_signup == 0, "No", "Yes")

knitr::kable(party_abcount, col.names = c("Group", "Signed up?", "Total"))
```

## Written Analysis


1. What does `treatment` represent in this experiment?
*Within the above experiment, 'treatment' refers to the random assignment of web traffic to two different political website homepages. A given value of 0 indicates the visitor saw the control page, while an individual with a value of 1 viewed a homepage that focused on a relevant hot-topic.*
2. What are `Y0` and `Y1`, conceptually?
*Y0 and Y1 represent potential outcomes. Y0 depicts the signup decision a visitor may have made having viewed the standard homepage. However, Y1 is the decision that the same visitor may have made upon viewing the issue-focused webpage. *
3. Why do we only observe one of `Y0` or `Y1` for each visitor? Why is this a problem for causality?
*Each visitor can only produce either Y0 or Y1 as they are only shown the assigned website homepage once. This is due to the problem of causal inference. As both outcomes cannot be viewed by a single individual, the individual treatments effects cannot be measured and causality is determined through averaging the outcomes of each group.*
4. What does `support_signup` represent in terms of potential and realised outcomes?
*'support_signup' represents the realised outcome, which is shown as a visitor's 'yes' or 'no' decision. This contrasts from the Y1 and Y0, as these represent the possible decisions a visitor may make whereas 'support_signup' is the actual result received from an individual that is recorded along with their assigned website homepage*


---

##  Signup Rates and Difference-in-Means ATE 

I will now estimate the average signup rate under each homepage version and compute the difference.


```{r group-means}
mean_control <- mean(party_ab$support_signup[party_ab$treatment == 0])
mean_treated <- mean(party_ab$support_signup[party_ab$treatment == 1])

means_df <- data.frame(
  treatment = c("Control Homepage", "Issue-Focused Homepage"),
  mean_signup = c(mean_control, mean_treated)
)

knitr::kable(means_df,
             col.names = c("Homepage Version", "Mean Signup Rate"),
             digits = 3)
```

##  Code: Group means and bar plot

```{r group-means-plot}
means_df $ se <- sqrt(means_df $ mean_signup * (1 - means_df $ mean_signup) /750)

ggplot(means_df, aes(x = treatment, y = mean_signup, fill = treatment)) +
  geom_col(width = 0.6) +
  geom_errorbar(aes(ymin = mean_signup - 1.96 * se,
                    ymax = mean_signup + 1.96 * se),
                width = 0.1,
                colour = "#000000") +
# Adding error bars
  geom_text(
    aes(label = paste0(round(mean_signup * 100, 1), "%")),
    vjust = -4
# Changing vjust for aesthetics
  ) +
  scale_y_continuous(limits = c(0, 0.22),
                     labels = scales::percent_format(accuracy = 1),
                     expand = expansion(mult = c(0, 0.05))) +
# removing padding from y axis
  scale_fill_manual(values = c("Control Homepage" = "#0072B2",
                               "Issue-Focused Homepage" = "#56B4E9")) +
# Colours change to preferred colour-blind friendly palette
  labs(
    x = NULL,
    y = "Signup rate",
    title = "Supporter Signup Rate by Homepage Version",
    subtitle = "Error bars representative of 95% confidence intervals of signup rates"
# Adding in subtitle and capitalisation of title
  ) +
  theme_grey() +
  theme(legend.position = "none")
```


## Code: Difference-in-means ATE

```{r diff-in-means}
# TODO: Compute the difference-in-means ATE
# Edited using Kable for aesthetics 
ate_hat <- mean_treated - mean_control
knitr::kable(ate_hat, col.names = NULL)
```

## Written Analysis

1. Interpret `mean_control` and `mean_treated` in plain language.
*'mean_control' signifies the basic signup rate, as it is the proportion of visitors who signed up after viewing the original website homepage.*
*On the other hand,'mean_treated' represent the test signups. It shows the proportion of visitors who joined upon viewing the hot-topic focused webpage. *
2. Convert `ate_hat` into percentage points . What does it mean, in words, for the party’s digital strategy?
*4.17%*
*The above percentage suggests that the party benefits by a 4.17% increase in signups through using their webpage to promote hot-topic issues to their web traffic.*
3. Ignoring uncertainty for the moment, would this result encourage the party to adopt the issue-focused homepage? Why or why not?
*Ignoring uncertainty, this result does encourage the party to adopt an issue-focused home page. As the primary goal of any digital marketing strategy is to increase the number of supporters, gaining 4.17 percentage points achieves this goal and is an improvement on the previous webpage.*
4. Change two things about this plot to make it more interesting/readable
*Upon initially viewing the plot, I altered the colours to suit my preferable colour-blind friendly palette. I then edited the capitalisation of the title and included a subtitle for the plot. In terms of direct changes to the plot itself, I included error bars and their percentages (which I adjusted to above the bars using vjust). Additionally, the y-axis was transformed from decimals into percentage points in order to increase the clarity and understand-ability of the plot. *

---

#  Linear Probability Model (lm) vs logit (glm) 

I will now estimate the ATE using our workhorse regression model.

## Code: Linear probability model

```{r lm-model}
lm_model <- lm(support_signup ~ treatment, data = party_ab)
coefs <- coef(lm_model)

tidy(lm_model, conf.int = TRUE) %>%
  mutate(term = recode(term,
                       "(Intercept)" = "Baseline (Control)",
                       "treatment" = "Campaign Effect (Treatment)")) %>%
  kable(digits = 4,
        col.names = c("Variable", "Estimate", "Std. Error", "t-value", "P-value",
                      "Lower CI", "Upper CI"))

glance(lm_model) %>%
  select(r.squared, adj.r.squared, sigma, p.value) %>%
  kable(digits = 4,
        col.names = c("R-Squared", "Adj. R-Squared", "Residual Std. Error",
                      "Model P-value"))

```

## Written Analysis

1. What does the **intercept** in `lm_model` estimate in this context?
*The model's intercept of 0.11851 represents the predicted mean of 'support_signup' for the control group.*
2. What does the **coefficient on `treatment`** estimate, and how does it relate to `ate_hat`?
*0.04170 is the coefficient for treatment which estimates the Average Treatment Effect (ATE). This coefficient represents the differences fo mean 'support_signup' between the control and the treatment groups. Within a linear model, the coefficient is identical to the estimated difference in mean, or 'ate_hat'. *
3. Check numerically that the `treatment` coefficient equals the difference in means from Section 2. Are they the same?
*Through the below code block, it can be seen that the 'treatment' coefficient equals the difference in mean as the use of the near() function returns TRUE. This means that the data rounds to the same number. *

```{r, Q3}
coeft <- coefs[2]
kable(c(near(coeft, ate_hat)), col.names = NULL)
```


## Code: Logistic regression (logit)

```{r logit-model}
logit_model <- glm(support_signup ~ treatment, data = party_ab,
                   family = binomial(link = "logit"))

tidy(logit_model) %>%
  kable(digits = 4,
        col.names = c("Term", "Estimate", "Std. Error", "z-value", "P-value"))
```

In the next section, I am going to go to some lengths to interpret these coefficients. Why would I have to do this?

*Extra effort is needed to interpret these coefficients as a linear probability model and a logit model have been used, therefore work is needed as these model outputs denote different 'units'. Ultimately, this is due to the use of log-odds within the logit model. Despite the extra effort, the trouble may be worth the extra effort as linear models are often preferred for their increased 'readability' whereas logit models are often considered more 'robust' as probabilities remain between 0 and 1. Additionally, through noticing the similarity of p-values between the models suggests that despite the difference in model, the statistical evidence for the treatment effect may remain consistent.*

## Code: Predicted probabilities under control and treatment

```{r logit-pred-probs}
p_hat_control <- predict( 
  logit_model,
  newdata = data.frame(treatment = 0),
  type = "response"
)
# Calculates predicted probability of control group signups

p_hat_treated <- predict(
  logit_model,
  newdata = data.frame(treatment = 1),
  type = "response"
)
# Calculates predicted probability of treatment group signups

ate_logit <- p_hat_treated - p_hat_control
# Calculates Average Treatment Effect for the logit model
# Uses probability scale so coefficients can be compared

kable(c("Control Probability" = p_hat_control,
        "Treated Probability" = p_hat_treated,
        "Logit ATE" = ate_logit),
      col.names = NULL, digits = 4)
```

## Written Analysis

1. Interpret `p_hat_control` and `p_hat_treated` in plain language.
*'p_hat_control' represents the predicted probability of signup for an individual from the control group. Whereas, 'p_hat_treated' represents the predicted probability of signup for an individual from the treatment group.*
2. Compare `ate_logit` to `ate_hat` and the `treatment` coefficient from `lm_model`. Are they similar or different? Why might they be close in this simple A/B test?
*ate_logit = 0.04170496*
*treatment coefficient = 0.04170496*
*ate_hat = 0.041705*
*The data above is identical (due to rounding). A simple A/B test with one binary predictor means that both the linear and logit model are considered as 'saturated', due to the mean calculation  of either group. Additionally, when converting logit models from log-odds to the probability scale, the predicted probabilities will match the OLS coefficient.*



---

#  Comparing lm and logit visually 

I will now visualise how the two models fit the data.

## Code: Plot with fitted probabilities

```{r plot-fitted, fig.width=6, fig.height=4}

# Prepare a data frame for group-level summaries
group_df <- data.frame(
  treatment = c(0, 1),
  mean_signup = c(mean_control, mean_treated),
  model = "LM group means"
)

logit_df <- data.frame(
  treatment = c(0, 1),
  mean_signup = c(p_hat_control, p_hat_treated),
  model = "Logit predicted probs"
)

# Main ggplot
ggplot(party_ab, aes(x = factor(treatment), y = support_signup)) +
  geom_jitter(
    width = 0.1,
    height = 0,
    alpha = 0.1,
    color = "black"
  ) +
  geom_line(
    data = group_df,
    aes(x = factor(treatment), y = mean_signup, group = 1, color = model),
    linewidth = 1
  ) +
  geom_point(
    data = group_df,
    aes(x = factor(treatment), y = mean_signup, color = model),
    size = 3
  ) +
  geom_point(
    data = logit_df,
    aes(x = factor(treatment), y = mean_signup, color = model),
    shape = 17,
    size = 3
  ) +
  scale_x_discrete(labels = c("Control", "Treatment")) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_color_manual(
    values = c(
      "LM group means" = "#0072B2",
      "Logit predicted probs" = "#D55E00"
    )
# Colours change to preferred colour blind-friendly palette
  ) +
  labs(
    x = "Treatment",
    y = "Support signup (0/1)",
    title = "Observed outcomes and fitted probabilities",
    color = NULL
  ) +
  theme_minimal()
```

## Written Analysis

Assuming I was hired as an analyst for the political party and I have to present these findings to my employer. The people that I am giving the presentation to do not have much quantitative experience. How might I show the information above in a way that is easy to understand?:

1. How different are the predicted sign-up probabilities from the two models for each group?
*Within the control group both models indicated a potential signup rate of ~11.85%. The treatment group models suggest a signup rate of ~16.02%. Thus whether utilising a linear or logit model, it was found that the treatment  increased the chance of an individual signing up by ~4.17% *
2. Does either model seem clearly inappropriate here, or do they both tell a similar story?
*Although using a linear model in many complex scenarios may be considered inappropriate due to the potential output of impossible probabilities, i.e. -5% or 115%. Within A/B testing, both the linear and logit models are considered 'saturated'. This means that they identify the averages across a specified group and therefore, as seen in the above plot, the outcomes are the same.*
3. Which model is preferable and why?
*In the context of presenting this information to individuals who may lack a complex understanding of quantitative methods, the preferred method would be the linear model. This model allows for an easier interpretation of the 4.17% increase as log-odds do not need to be explained. Therefore  increasing clarity for more intuitive decision-making as well as saving time and effort.*

---

# Heterogeneous Effects by Age or Interest (Interaction)

Now, I am going to add simulated covariates. The two variables I will make are age and high political interest.
Assuming visitors are between 18 and 75, and age is roughly distributed normal around 40

```{r het-effects}
party_ab$age <- round(pmin(pmax(rnorm(nrow(party_ab), mean = 40, sd = 12), 18), 75))
# Adding simulated age variable, specifying normal distribution
# Ages are between 18 and 75, only whole numbers are included

# High political interest: 1 = high interest, 0 = low interest (about 40% high)
party_ab$high_interest <- rbinom(nrow(party_ab), size = 1, prob = 0.4)

knitr::kable(head(party_ab[, c("visitor_id", "treatment", "support_signup", "age",
                               "high_interest")]))
```

Next, I am going to run an interaction with treatment and interest

```{r het-interest}
lm_het_interest <- lm(support_signup ~ treatment * high_interest, data = party_ab)

tidy(lm_het_interest) %>%
  kable(digitd = 4,
        col.names = c("Term", "Estimate", "Std. Error", "t-value", "P-value"))
```

# Written Analysis:

1. What is the implied treatment effect by ``interest`` subgroup. Note that interest is coded 0,1.
*0.044001 + (-0.006143) = 0.037858*
*The treatment appears to be slightly more effective for individuals with a low political interest, however due to the interaction term nor being significant to the standard 0.05, there may be more analysis necessary.*
2. What is the treatment effect when age = 0? Is this a meaningful baseline?
*When age = 0, the treatment effect is estimated by the treatment coefficient of 0.04170496. Despite the significance of this output, it is not a meaningful number as the simulated age for voters lies between 18 and 75 which makes this an extrapolation of data that does not represent an individual or set of people within this data set. Additionally, 0-year-olds would be unable to see the treatment or control groups and also lack the cognitive ability to signup, even if they were legally allowed to vote. *
3. Which variable gives  information of how the treatment effect changes for each additional year of age? How do would they be interpretted?
*This would be the interaction term which is able to denote whether the campaign is increasing in yield or decreasing as age increases. If an interaction term is positive, it means that the effectiveness of treatment is increasing with the age of the individual, however a negative term means that the treatment is losing impact, or decreasing, as age increases. *
4. Plot the treatment effects at specific ages (e.g., 25, 40, 60).

```{r}
lm_age <- lm(support_signup ~ treatment * age, data = party_ab)

target_ages <- c(18, 25, 40, 60, 75)
# Specifying X axis ticks

plot_data <- expand.grid(age = target_ages, treatment = c(0,1))
plot_data $ prob <- predict(lm_age, newdata = plot_data)

lift_summary <- plot_data %>%
  pivot_wider(names_from = treatment, values_from = prob, names_prefix = "grp_") %>%
  mutate(lift = grp_1 - grp_0)
# Creating table before creating visualisation

knitr::kable(lift_summary, digits = 4,
             col.names = c("Age", "Control Rate", "Treatment Rate", "Total Lift"))


custom_y_breaks <- c(0, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225)
# Ensuring uniform tick breaks, omitting unnecessary ticks

uniform_gridlines <- seq(0, 0.225, by = 0.025)
# Adjusting background gridlines

ggplot(plot_data, aes(x = age, y= prob, colour = factor(treatment))) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  scale_x_continuous(breaks = c(18, 25, 40, 60, 75)) +
  scale_y_continuous(labels = percent_format(accuracy = 0.1),
                     limits = c(0, 0.225),
                     breaks = custom_y_breaks,
                     minor_breaks = uniform_gridlines,
                     expand = c(0, 0)) +
# Axial edits to increase readability
  scale_colour_manual(values = c("1" = "#D55E00", "0" = "#E69F00"),
                      name = "Legend",
                      breaks = c("1", "0"),
                      labels = c("Treatment", "Control")) +
# Specifying preferred colour blind-friendly palette, editing legend
  labs(title = "Campaign Impact by Voter Age",
       subtitle = "Showing signup probability of research conditions with age",
       x = "Age (Yrs)",
       y = "Signup Probability") +

# Adding labels and title/subtitle
  theme_grey() +
  theme(panel.grid.major = element_line(colour = "white"),
        panel.grid.minor.y = element_line(colour = "white"),
        panel.grid.minor.x = element_blank())
# Editing background gridlines for aesthetics
```



# Ethical Reflection on Political A/B Testing

So far, I have treated this like a standard A/B test. But here the product is a political message, and the outcome is political support.

What could be some potential **ethical considerations** that may arise when a political party runs an A/B test on website visotors?

*Through conducting A/B testing, in the eyes of analysts, the voters are transformed into data points which raises some significant ethical concerns in terms of informed consent. Unlike with usual methods and fields of research, conducting experiments through the use of political messaging can create a direct impact on civic beliefs and election outcomes. The use of statistical models that isolate 'high interest' voters may enable political parties to micro-target specific groups of individuals, perhaps causing filter bubbles in which differing target groups receive alternate contradictory promises. Furthermore, utilising this practice may deteriorate shared factual political realities through a prioritisation of persuasion over public interaction.*
*In addition, although Average Treatment Effect (ATE) can measure mathematical efficiency, it does not account for the contents of an imposed treatment condition. For example, using a 4.17% lift as a method of justification for fear-based messaging or misinformation underscores an intense conflict between statistical successes and real-world moral responsibilities. As website traffic would likely be unaware of their participation in online behavioural experiments, this treatment condition may end up shifting an individual’s political supports and thus the lack of transparency may adjust just persuasion toward psychological engineering.*
*In consequence, there is a risk of immorality within political parties that incorporate unwitting internet traffic into research may prioritise percentage lift over the necessity of informed consent and participation. An approach such as this may be viewed as conversion-focused and thus may reduce democratic transparency to data-driven transactions, perhaps erosion the transparency of which public trust is built upon.*
